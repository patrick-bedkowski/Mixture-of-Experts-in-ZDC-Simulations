# Generative DNN for Physics Simulations CERN

## Table of contents

1. [Setting Developing Environment](#setting-developing-environment)
2. [Producing data for tests](#producing-data-for-tests)
    1. [Original files](#original-files)
    2. [Data filtering](#data-filtering)

## Setting Developing Environment

In order to run all experiments and analysis jupyter notebooks, it is necessary to install the following tools: <br />Download version for your operating system.
1. [Python 3.9.16 download](https://www.python.org/downloads/release/python-3916/)
2. Install CUDA: <br />
    Note, for your system to actually use the GPU, it nust have a [Compute Capibility](https://developer.nvidia.com/cuda-gpus) >= to 3.0<br />
    Install CUDA 11.7 for your OS
   1. [CUDA Toolkit 11.7 Downloads](https://developer.nvidia.com/cuda-11-7-0-download-archive)<br />
   * Windows:<br /> double-click the executable and follow setup instructions<br />
   * Linux:<br /> follow the instructions [here](http://askubuntu.com/a/799185)<br />
   2. [Download cuDNN v8.9.6 (November 1st, 2023), for CUDA 11.x](https://developer.nvidia.com/rdp/cudnn-archive)
3. Install python pip modules from `requirements.txt` using command:
```pip install -r requirements.txt```

After the above setup it should be possible to run the scripts.


## Producing data for tests

In order to run all experiments, it is necessary to build datasets from original files.

Necessary data to run the experiments are the following:
- dataset with 9 conditional variables describing the: Mass, Energy, Charge, 3 vectors for momenta and 3 vectors for coordinates
  - available under this link: [data_cond](https://anonymized-data-for-review.s3.eu-west-1.amazonaws.com/data_cond_photonsum_proton_1_2312.pkl)
- dataset with images originating from Proton ZDC device
  - available under this link: [data](https://anonymized-data-for-review.s3.eu-west-1.amazonaws.com/data_proton_photonsum_proton_1_2312.pkl) 
- dataset with images originating from Neutron ZDC device
  - available under this link: [data_coord](https://anonymized-data-for-review.s3.eu-west-1.amazonaws.com/data_coord_proton_photonsum_proton_1_2312.pkl) 

### Original files

Original files were generated by the [GEANT4](https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.13048).
Instructions on how to generate data are available in [here](https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideSimulation).<br />
The below files explain the order of steps that need to be performed to run the experiments.

### Run training

In order to facilitate training, one need to download input datasets needed for the simulation and save them locally. Later
modify these variables and set the path to the datasets in the `execute_training_expertsim_proton.py` file:

```python
DATA_IMAGES_PATH = "data_proton_photonsum_proton_1_2312.pkl"
DATA_COND_PATH = "data_cond_photonsum_proton_1_2312.pkl"
DATA_POSITIONS_PATH = "data_coord_proton_photonsum_proton_1_2312.pkl"
```

If you wish to see the training progress in Weights&Biases webtool, you can set the variable `SAVE_EXPERIMENT_DATA = True`, and in the following
lines input your wandb data account:

```python
run = wandb.init(
    project="<your-project-name>",  # create project on W&B web tool and input its name here
    entity="<your-profile-name>",
    name=wandb_run_name,
    config=config_wandb,
)
```

Then run the python file:

```python
python3 execute_training_expertsim_proton.py
```